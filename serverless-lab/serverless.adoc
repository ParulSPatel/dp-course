== Serverless Lab

Through the last four labs we've gone from hand-cranking our infrastructure, to configuring it with imperative scripts, to declaring it in declarative fashion with tools like packer and terraform. We've also seen, at each iteration, how our infrastructure becomes more disposable, more immutable, and less precious. That hand-cranked server with no definition in code? It would be a shame to lose that since reproducing it could be a lengthy manual process. But a docker container in our Kubernetes cluster? Who cares? The system will simply spin up another one to meet the demands of our service and wire it into our existing infrastructure. And that container will look just like all the others in its cluster.

Systems like Kubernetes (or containers in general) have allowed IT shops to more rapidly deploy changes to the systems that run code. Pushing out a new container definition to your Kubernetes cluster will see relatively rapid propagation as existing containers as Kubernetes deploys your new image and tears down the old one. But there are limits to this. Docker containers still run on servers. Building and pushing out a new server image to a large fleet is a much more time-intensive process than pushing up a new container definition, even when that push is automated. And the servers running our container cluster will indeed need to be updated and replaced from time to time.

What if there were a way not only to deploy your infrastructure almost as quickly as your new code, but to drastically simplify the amount of infrastructure management you as the IT professional need to do? This is the idea behind **Functions as a Service (FaaS)**.

FaaS abstracts away nearly the entire compute layer and hands it off to someone else (likely a major public cloud provider like Amazon, Google or Microsoft). You write a relatively simple definition for your function indicating things like:

* how much memory to use
* which code the function should run
* what permissions the function should have.

You then deploy this definition and the cloud provider takes care of the rest.

There is almost no wait for an updated infrastructure definition to match the new code it's running - new calls to the function use your new definition almost immediately. The declarative code describing your function is also drastically simpler than the definition of a server and container array. Finally, there is the potential for a massive cost savings since most cloud providers only charge you when your function is invoked (run). This is unlike cloud servers which, while they add and subtract instances to match demand, are always running and thus costing money.

Let's deploy our first function as a service.



Before we can start, we need to enable a few new google apis to ensure no interruptions during our terraform create. Run the following command:

[source,bash]
----
$ gcloud services enable\
    cloudbuild.googleapis.com \
    cloudfunctions.googleapis.com \
    logging.googleapis.com \
    pubsub.googleapis.com \
    storage-api.googleapis.com \
    storage-component.googleapis.com
----

If you discover you're still missting a permission during the terraform deploy, you'll receive an error notice with a link to enable the missing API. Simple enable and then run your deploy agian. There may also be a delay between when you activage and API and when Google begins allowing its use. This is usually only a minute or two at most.


Once terraform indicates your function is deployed, set your function's name as an environment variable with this command:

[source,bash]
----
$ export FUNCTION=$(gcloud functions list --format="value(httpsTrigger.url)")
----